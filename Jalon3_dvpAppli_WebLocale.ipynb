{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install contractions "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qdt59p7TMk5g",
        "outputId": "c73d31bd-3361-4108-cdb4-92b1a70ed75e"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.72)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jalon 3 : Développement application web locale\n"
      ],
      "metadata": {
        "id": "Tjo5KBtrjSFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import en_core_web_sm\n",
        "import nltk\n",
        "import contractions\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from pickle import *\n",
        "from textblob import TextBlob\n",
        "from wordcloud import WordCloud\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from spacy.lang.en.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "2aaB228WlMvj"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "63XzS65nO1Sm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e038bcf0-2e17-422b-b5ce-5dcb6ce2e7d6"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    text_processed = \" \".join(tokenizer.tokenize(text))\n",
        "    return text_processed"
      ],
      "metadata": {
        "id": "0IaxWP_1LnA6"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = en_core_web_sm.load(disable=['parser', 'tagger', 'ner'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    \n",
        "    tokens_tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemmatized_text_list = list()\n",
        "    \n",
        "    for word, tag in tokens_tagged:\n",
        "        if tag.startswith('J'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'a')) # Lemmatise adjectives. Not doing anything since we remove all adjective\n",
        "        elif tag.startswith('V'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'v')) # Lemmatise verbs\n",
        "        elif tag.startswith('N'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'n')) # Lemmatise nouns\n",
        "        elif tag.startswith('R'):\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word,'r')) # Lemmatise adverbs\n",
        "        else:\n",
        "            lemmatized_text_list.append(lemmatizer.lemmatize(word)) # If no tags has been found, perform a non specific lemmatisation\n",
        "    \n",
        "    return \" \".join(lemmatized_text_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMNeoC45MRfK",
        "outputId": "561a8b75-55b5-4257-b16e-f8fe6b98aa42"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['parser', 'tagger', 'ner'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    return \" \".join([word.lower() for word in text.split()])"
      ],
      "metadata": {
        "id": "p5qM4UUVNRE1"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contraction_text(text):\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "VaraSy2ANblO"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words = ['not', 'no', 'never', 'nor', 'hardly', 'barely']\n",
        "negative_prefix = \"NOT_\"\n",
        "\n",
        "def get_negative_token(text):\n",
        "    tokens = text.split()\n",
        "    negative_idx = [i+1 for i in range(len(tokens)-1) if tokens[i] in negative_words]\n",
        "    for idx in negative_idx:\n",
        "        if idx < len(tokens):\n",
        "            tokens[idx]= negative_prefix + tokens[idx]\n",
        "    \n",
        "    tokens = [token for i,token in enumerate(tokens) if i+1 not in negative_idx]\n",
        "    \n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "j0o7GgpWNgUq"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "    english_stopwords = stopwords.words(\"english\") + list(STOP_WORDS) + [\"tell\", \"restaurant\"]\n",
        "    \n",
        "    return \" \".join([word for word in text.split() if word not in english_stopwords])"
      ],
      "metadata": {
        "id": "FWlbpYtKNjuK"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \n",
        "    # Tokenize review\n",
        "    text = tokenize_text(text)\n",
        "    \n",
        "    # Lemmatize review\n",
        "    text = lemmatize_text(text)\n",
        "    \n",
        "    # Normalize review\n",
        "    text = normalize_text(text)\n",
        "    \n",
        "    # Remove contractions\n",
        "    text = contraction_text(text)\n",
        "\n",
        "    # Get negative tokens\n",
        "    text = get_negative_token(text)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    text = remove_stopwords(text)\n",
        "    \n",
        "    return text  "
      ],
      "metadata": {
        "id": "i-EoxXx0NrDd"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Partie 1 : Prédiction d'un sujet d'insatisfaction"
      ],
      "metadata": {
        "id": "4be6RxzpjcXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importer les fichiers pickles 'model' et 'vectoriseur' \n",
        "\n",
        "model_pickle = open (\"/content/drive/MyDrive/E3/TLN/modelEntraineLehna\",'rb')\n",
        "modelEntraine = load(model_pickle)\n",
        "print(modelEntraine)\n",
        "\n",
        "vectoriseur_pickle = open (\"/content/drive/MyDrive/E3/TLN/vectoriseurLehna\",'rb')\n",
        "vectorizer = load(vectoriseur_pickle)\n",
        "print(vectorizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeuIM-aakOQ9",
        "outputId": "6dbf5a14-3253-48c0-b032-d7c30bf61361"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NMF(n_components=15)\n",
            "TfidfVectorizer(max_df=0.8, min_df=0.01)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model,vectorizer,n_topic,new_reviews):\n",
        "  new_reviews = preprocess_text(new_reviews)\n",
        "  blob=TextBlob(new_reviews)\n",
        "  sentimentBlob=blob.sentiment.polarity\n",
        "  new_reviews = [new_reviews]\n",
        "  new_reviews_transformed=vectorizer.transform(new_reviews)\n",
        "\n",
        "  prediction= model.transform(new_reviews_transformed)\n",
        " \n",
        "  topics=[ 'Cadre du lieu',\n",
        "           'Plats en sauce',\n",
        "           'Menu pizza ',\n",
        "           'Service livraison et commandes',\n",
        "           'Qualité des plats ',\n",
        "           'Qualité du service',\n",
        "           'Menu burger',\n",
        "           'Temps attente',\n",
        "           'Menu chicken',\n",
        "           'Service bar ',\n",
        "           'Localisation du lieu',\n",
        "           'Relation client',\n",
        "           'Menu sandwich',\n",
        "           'Menu sushis',\n",
        "           'Clients revenus']\n",
        "\n",
        "  if sentimentBlob<0 and sentimentBlob>-1:\n",
        "    max = np.argsort(prediction)\n",
        "    max_list=(list(max[0]))\n",
        "    max_list.reverse()\n",
        "    print(max_list)\n",
        "    topic=[]\n",
        "    for i in range(n_topic):\n",
        "      topic.append(topics[max_list[i]])  \n",
        "    return sentimentBlob,prediction,topic\n",
        "\n",
        "  return sentimentBlob"
      ],
      "metadata": {
        "id": "9Wa2nx-qmSYb"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_reviews = \"I dont like chicken \"\n",
        "prediction(modelEntraine,vectorizer,2,new_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTyIyjU44B7u",
        "outputId": "42122fb6-2540-424c-bc9d-d7c5559495ee"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 14, 13, 12, 11, 10, 9, 7, 6, 5, 4, 3, 2, 1, 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.6,\n",
              " array([[0.       , 0.       , 0.       , 0.       , 0.       , 0.       ,\n",
              "         0.       , 0.       , 0.1314366, 0.       , 0.       , 0.       ,\n",
              "         0.       , 0.       , 0.       ]]),\n",
              " ['Menu chicken', 'Clients revenus'])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOPICS = ({     0 :'Cadre du lieu',\n",
        "                1 :'Plats en sauce',\n",
        "                2 :'Menu pizza ',\n",
        "                3 :'Service livraison et commandes',\n",
        "                4 :'Qualité des plats ',\n",
        "                5 :'Qualité du service',\n",
        "                6 :'Menu burger',\n",
        "                7 :'Temps attente',\n",
        "                8 :'Menu chicken',\n",
        "                9 :'Service bar ',\n",
        "                10:'Localisation du lieu',\n",
        "                11:'Relation client',\n",
        "                12:'Menu sandwich',\n",
        "                13:'Menu sushis',\n",
        "                14:'Clients revenus'\n",
        "          })\n",
        "\n",
        "def predict_topics(model, vectorizer, n_topics, text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity < 0:\n",
        "        text = preprocess_text(text)\n",
        "        text = [text]\n",
        "        vectorized = vectorizer.transform(text)\n",
        "        topics_correlations = model.transform(vectorized)\n",
        "        unsorted_topics_correlations = topics_correlations[0].copy()\n",
        "        topics_correlations[0].sort()\n",
        "        sorted = topics_correlations[0][::-1]\n",
        "        print(sorted)\n",
        "        topics = []\n",
        "        for i in range(n_topics):\n",
        "            corr_value = sorted[i]\n",
        "            result = np.where(unsorted_topics_correlations == corr_value)[0]\n",
        "            topics.append(TOPICS.get(result[0]))\n",
        "        print(topics)\n",
        "        return polarity\n",
        "    else:\n",
        "        return polarity"
      ],
      "metadata": {
        "id": "36jHgHWs3ttj"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_reviews = \"I dont like chicken \"\n",
        "predict_topics(modelEntraine,vectorizer,2,new_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdwFGUY93w0P",
        "outputId": "5e845bc0-e087-4c6f-dd4d-a39a9dc6bf9e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.1314366 0.        0.        0.        0.        0.        0.\n",
            " 0.        0.        0.        0.        0.        0.        0.\n",
            " 0.       ]\n",
            "['Menu chicken', 'Cadre du lieu']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but NMF was fitted with feature names\n",
            "  \"X does not have valid feature names, but\"\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.6"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ]
}